{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Visualization-in-rviz\" data-toc-modified-id=\"Visualization-in-rviz-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Visualization in rviz</a></span></li><li><span><a href=\"#Simultaneous-Localization-and-Mapping-(SLAM)\" data-toc-modified-id=\"Simultaneous-Localization-and-Mapping-(SLAM)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Simultaneous Localization and Mapping (SLAM)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Gmapping-with-the-Turtlebot-3\" data-toc-modified-id=\"Gmapping-with-the-Turtlebot-3-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Gmapping with the Turtlebot 3</a></span></li></ul></li><li><span><a href=\"#Rosbag-files\" data-toc-modified-id=\"Rosbag-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Rosbag files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recording-bag-files\" data-toc-modified-id=\"Recording-bag-files-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Recording bag files</a></span></li><li><span><a href=\"#Playing-bag-files\" data-toc-modified-id=\"Playing-bag-files-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Playing bag files</a></span></li><li><span><a href=\"#Turtlebot-example\" data-toc-modified-id=\"Turtlebot-example-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Turtlebot example</a></span></li></ul></li><li><span><a href=\"#Navigation-stack\" data-toc-modified-id=\"Navigation-stack-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Navigation stack</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on with the Turtlebot 3\n",
    "\n",
    "This final part of the workshop is focused on gaining practical experience with ROS. To this end, we will make use of the new, low-cost robot platform [Turtlebot 3](http://www.turtlebot.com/). This is the flagship robot of ROS. We use the third iteration of this platform, more specifically the burger model. It is comprised of a few main components:\n",
    "\n",
    "- **LIDAR** : on the very top of the Turlebot, you will find a 360° laser scanner. This sensor transmits a rotating laser, which reflects of nearby obstacles. By using the reflection time, distance to the obstacles can be obtained. These distances can be used to build a map of the environment.\n",
    "- **Raspberry Pi**: underneath the LIDAR you will find a Raspberry Pi 3. This credit card-sized computer runs a special version of ubuntu linux. Since its introduction a few years ago, the Raspberry Pi has gained incredible popularity among hobbyists and researchers due to its small size, low cost and power use. It is most usefull in applications that need a computer, but not a huge amount of processing power. On the Turtlebot, its job is to read data from the sensors and communicate this with a 'master' pc, which does the actual computations.\n",
    "- **OpenCR**: just above the battery and the motors, you will find the hardware control board. It contains an onboard IMU with a three axis accelerometer, gyroscope and magnetometer. Its main function is to connect the raspberry pi to the motors and the sensors.\n",
    "\n",
    "![turtlebot3_burger](http://emanual.robotis.com/assets/images/platform/turtlebot3/hardware_setup/turtlebot3_burger_components.png)\n",
    "\n",
    "In the following sections, you will learn how to visualize data published on ROS topics with rviz. Next, we will discuss rosbag, a tool that allows you to record data from ROS topics and replay it later (without a robot). Finally, you will use the navigation stack to let your robot drive autonomously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization in rviz\n",
    "\n",
    "rviz is a 3D visualization tool for ROS. It is included in a full desktop install of ROS, but can of course be installed separately as well. rviz can display robot positions (with a complete robot model if it is provided in a URDF file), laser scans, point clouds and much more information that is being published by ROS nodes. To start rviz, a roscore needs to be running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!roscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In a separate terminal, you can start rviz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the left mouse button to rotate the camera, the right mouse button to zoom and the middle mouse button \n",
    "to pan. On the left hand side of the window the display options are listed. The global options are listed at the top. Parameters like background color and frame rate can be adjusted here. There is also an option called 'Fixed Frame', which referes to the static coordinate frame relative to which all other information is referenced. By default, the frame 'odom' is selected, but 'base_footprint', 'world' and 'map' are also often used. The fixed frame should not be moving (e.g. it should not be attached to a mobile robot), doing so might result in erroneous visualization. \n",
    "\n",
    "Below the global options, you can adjust the settings of the visualization grid, such as the cell size and the color. Below that, you will find the only topic that is shown by default, namely the /tf topic. tf is an abbreviation for transfer frame, and contains information about the relative location of all coordinate frames in the system. Eventually, all frames should be directly or indirectly referenced relative to the fixed frame, thus building a so called 'tf-tree'.\n",
    "\n",
    "At the bottom left of the rviz window, the 'Add' button allows you to visualize additional topics. By selecting the LaserScan message type for example, an additional display option will appear. You can then set parameters like the laserscan topic and the size and color of the markers. You can add additional display options in a similar manner. You can even have two display options of the same type (for example, 2 camera displays). This option only makes sense when the data being displayed originates from different topics though.\n",
    "\n",
    "Once you have added the desired display options, you can save your configuration as a .rviz file. Doing so will allow you to have the exact same visualization window in the future (and thus prevents having to add the same topics over and over again). To save your rviz file:\n",
    "\n",
    "1. Select the 'file' option in the window bar of the rviz panel. \n",
    "2. Select 'save config as'. \n",
    "3. Choose an appropriate name and location for your file. Select the default location (/home/ROS-workshop/.rviz) and 'example' as a filename to follow along with the rest of the chapter.\n",
    "\n",
    "You can now safely close rviz. To open the same configuration file in rviz, you can either type the ```rviz``` command in a terminal, then navigate to the file be selecting 'file' and then 'open'. Alternatively, you can pass the configuration file as an argument when opening rviz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rviz rviz  -d $HOME/.rviz/example.rviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument after ```-d``` should be the path to the rviz file. If everything works correctly, you should see the display options you added earlier in stead of the default window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultaneous Localization and Mapping (SLAM)\n",
    "\n",
    "The SLAM (Simultaneous Localization and Mapping) problem is the problem of building a map of the environment while simultaneously determining the robot’s position relative to this map. This problem is also referred to as Concurrent Mapping and Localization (CML). A mobile robot moves through an unknown environment, starting at a known location (often given by coordinates {0,0,0}). Its motion is uncertain due to the robot’s model and sensor noise.\n",
    "As it roams, the robot can detect the environment using a form of observation measurement. To compute the map and the robot’s location within this map, the probability of accuracy of the data must be included. \n",
    "\n",
    "SLAM has received considerable attention since the 1990's. Several approaches have been proposed such as Kalman and Particle filter SLAM. Gmapping is arguably the most popular software implementation of SLAM. It employs a Rao-Blackwellized particle filter (RBPF) and builds a 2D map based on laser scan data. The code and documentation are available on the [OpenSLAM website](http://openslam.org/gmapping.html). It is very easy to integrate into ROS, and can even be run on a low cost robot-platform like our Turtlebots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gmapping with the Turtlebot 3\n",
    "We will now move on to a more practical example of SLAM in ROS. To this end, we will build a map using gmapping and a Turtlebot 3 robot. This robot has an onboad computer, namely a raspberry pi 3 (RPi). The RPi has sends commands to the motors and reads out the various sensors. It relays this information to a remote pc, which does the actual processing. The RPi's on our robots have been configured to function as an access point, which means that a connection from your computer to the robot can be established just like any other wifi-connection. There is no central connection point, and thus no central point of failure. \n",
    "\n",
    "Before we can move on to the actual mapping, a roscore needs to be started on the robot, which will listen to the roscore on your PC in a master/slave configuration.\n",
    "\n",
    "1. Flip the switch at the back of the Turtlebot to turn it on.\n",
    "2. On the remote PC (master) connect to the Turtlebot 3 wifi (password = groupRBP). It can take a few minutes for the robot to show in the available wireless connections.\n",
    "3. Determine your own ip-address with the command ```ifconfig``` and write it down for later.\n",
    "3. Establish an SSH (secure shell) connection:\n",
    "    1. Open a terminal and type: ssh rosberrypi@192.168.42.1\n",
    "    2. When prompted for a password, type: *groupRBP*. Unlike most applications, you will not see any symbols while typing the password. However, the keys are still being registered. So type the password and hit enter.\n",
    "    3. If you are asked to add rosberrypi to the list of known hosts, type 'yes' and press enter\n",
    "    4. If everything went as expected, the user in the terminal should have changed. All your commands should now be preceded by *'rosberrypi@rosberrypi'*. Any command you now type in this terminal is executed on the raspberry pi.\n",
    "4. Edit the .bashrc file of both your PC and the Turtlebot with the command ```sudo nano ~.bashrc``` according to the configuration below. The passwords of the ROS workshop PC's and raspberry pi's are ROS and groupRBP, respectively (pay attention to the capital letters). ![network configuration](http://emanual.robotis.com/assets/images/platform/turtlebot3/software/network_configuration.png)\n",
    "4. Start a roscore on the pi with the following command: ```bringup```. Besides starting a regular roscore, communication with the LIDAR and motors is now initialized as well.\n",
    "5. To test that everything is working correctly, open a new terminal and type the following command on your own PC:\n",
    "```roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch```. To open terminal in a new tab of the same window, press CTRL+SHIFT+T.\n",
    "6. You should now be able to drive the Turtlebot around with your keyboard. When you are done, end the process with CTRL+C. Leave the other two terminals open.\n",
    "\n",
    "We are now ready to start gmapping. Open a new terminal and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!roslaunch workshop_tutorial turtlebot_slam.launch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start gmapping and rviz. You can steer the Turtlebot with your keyboard. The map will get updated and shown in rviz as you drive. \n",
    "\n",
    "![Gmapping example](images/gmapping_example.png)\n",
    "\n",
    "Gmapping builds an occupancy grid type map, which means explored terrain is either marked as free space or obstacles. Areas marked in white are considered free space where the robot can drive. Black is used to mark the location of (static) obstacles. Grey areas are unexplored.\n",
    "\n",
    "To save the map run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rosrun map_server map_saver -f ~/ROS_workshop/data/map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save the map in the subdirectory 'data' of the 'ROS_workshop' directory wth the filename 'map'. The map will be saved in both .pgm and .yaml format. The .pgm file is mostly for visual inspection. The .yaml file can be used for autonomous navigation, which we will test in the navigation stack section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rosbag files\n",
    "\n",
    "One of the nice features of ROS is that subscribers do not care about who is publishing. This means that data can originate from a physical system (such as a robot) a simulation (such as Gazebo) or even from recorded experiments.  Rosbag is a tool for recording and playing back data from a ROS system. A rosbag file is a specialized file format that contains the messages from one or several ROS topics. API's exist for use in the [command line](http://wiki.ros.org/rosbag/Commandline), [C++](http://wiki.ros.org/rosbag/Code%20API#cpp_api) and [python](http://wiki.ros.org/rosbag/Code%20API#py_api). The recording process is a node itself, which means that it can also be started from inside a launch file. The rest of this tutorial will focus on the command line interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recording bag files\n",
    "\n",
    "To record one or more topics currently being published, simply run rosbag record, for example the following command will record the '/tf' and '/cmd_vel' topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rosbag record tf cmd_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording all topics is accomplished with the -a argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rosbag record -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default name of the file is the current UNIX timestamp. To specify a different name, use the -O or -output name. The following command will create a bag file called chatter.bag with the messages from the /chatter topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rosbag record -O chatter.bag /chatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing bag files\n",
    "\n",
    "Playing back recorded data is accomplished by using 'rosbag play'. A roscore already has to be running for this command to work. For example to play back data from a file called 'recorded1.bag' run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rosbag play recorded1.bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turtlebot example\n",
    "\n",
    "As an example, we will now record the results of gmapping with the Turtlebot. If you already closed the previous terminals:\n",
    "\n",
    "1. Start a ```roscore``` on your PC\n",
    "2. Execute ```bringup``` on the raspberry pi via SSH\n",
    "3. Instead of the regular slam command, we will now specify that we want the results to be recorded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!roslaunch workshop_tutorial turtlebot_slam record:=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again you are able to drive the Turtlebot with the WASD keys whilst it is building a map of the environment. With the record argument set to True, however, results will now be saved as a rosbag file in the 'data' subdirectory inside the 'ROS_workshop' folder. Drive around with the Turtlebot untill you have mapped a few meters of the environment. If you forgot to save the map during the last Turtlebot mapping test, or if the enviroment has changed significantly since then (due to for example the movement of chairs), go ahead and save it now. We will need it in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rosrun map_server map_saver -f ~/ROS_workshop/data/map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the map, you can stop the mapping with CTRL+C. Leave the other terminals open for now. Next, navigate to the folder where the bagfile is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd ~/ROS_workshop/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you list the files in that folder with ```ls```, you should see a folder 'example' and a file with a recent timestamp as filename, and .bag as an extension (example 2018-03-29-14-30.bag). To learn more about the contents of the bagfile before playing it, you can use rosbag info. With the filename from our example the command becomes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rosbag info 2018-03-29-14-30.bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the topics listed for which messages were recorded, alond with the number of messages for each topic. Some additional metadata such as the duration and size of the bagfile are also listed. You can also use a graphical tool if you prefer this over the command line interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rqt_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the contents of a bag file, click on the folder icon in the top-left corner and locate the file you whish to inspect. Double click the file to view its contents. You should see a window similar to the one below: ![rqt_bag window](images/rqt_bag.jpg). On the left hand side, the topics contained in the bagfile are listed. Next to the topics is a timeline. The blue lines in the timeline indicate that a message was recorded at this moment. By right-clicking on a topic and then selecting 'view' and finally 'plot', you can see a 2D plot of a property of the published messages. The figure below illustrates the plotting of the x-velocity in a [Twist message: ](http://docs.ros.org/api/geometry_msgs/html/msg/Twist.html) ![rqt_bag plot](images/rqt_bag_plot.jpg)\n",
    "\n",
    "rqt_bag can be used to play the contents of rosbag file with the play and pause buttons in the top of the window. Generally though, it is more interesting to launch a rosbag player from inside a launch file. This then enables you to test a different algorithm or an update to your software with the exact same measurements more easily, as you can simply launch all the required nodes from the launch file. So close the rqt_bag window and run the turtlebot launch file with the play argument set to true. Don't forget to change the name of your bag file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!roslaunch workshop_tutorial turtlebot_slam play:=True bagfile:=2018-03-29-14-30.bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the map being built like before, except now the robot should not be moving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation stack\n",
    "\n",
    "The [ROS navigation stack](http://wiki.ros.org/navigation) is a collection of packages that enable autonomous operation for ROS-enabled robots. Sensor data such as odometry and laser scans is taken as an input, and actuator commands are sent to a mobile robot. Succesfull use of the navigation stack depends on a number of hard and software requirements.\n",
    "\n",
    "Software requirements:\n",
    " - The robot must be controlled via a ROS master\n",
    " - A [tf](http://wiki.ros.org/tf) transform tree must exist which describes the relation of the robot joints.\n",
    " - Sensor data must be published in the right message types.\n",
    " - The navigation stack must be configured according to the shape and dynamics of the robot.\n",
    " \n",
    " Hardware requirements:\n",
    "- The stack is meant for both differential drive and holonomic wheeled robots only. It assumes that the mobile base is controlled by sending desired velocity commands to achieve in the form of: x velocity, y velocity, theta velocity.\n",
    "- A planar laser mounted somewhere on the mobile base is required. This laser is used for map building and localization.\n",
    "- The Navigation Stack was developed on a square robot, so its performance will be best on robots that are nearly square or circular. It does work on robots of arbitrary shapes and sizes, but it may have difficulty with large rectangular robots in narrow spaces like doorways.\n",
    "\n",
    "A list of robots which use the navigation stack can be found on the [ROS wiki](http://wiki.ros.org/navigation/RobotsUsingNavStack). Fortunately, the Turtlebot 3 is on that list. As such we can use it to get some hands-on experience. Make sure you have a yaml file with the name 'map' in the ROS_workshop/data folder. Additionally, the clock of the Turtlebot and your PC need to be approximately synchronized. Unfortunately the raspberry pi does not have a real time clock, which means that the clock resets every time the pi reboots. Therefore we first need to synchronize the clocks. Open a new terminal and navigate to the ROS_workshop folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cd ~/ROS_workshop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will execute a so called 'shell script'. This is simply a set of instructions that run in your terminal one after the other. You can automate a lot of tasks with shell scripts such as copying files, pinging servers, etc.  Our shell script will save the current date, send it to the pi over SSH, and set that as the pi's current date. \n",
    "\n",
    "A shell script is run by prefacing the name of the script with './' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./update_clock.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to type the password of the raspberrypi (groupRBP) twice during executing. As usual, you will not see any feedback such as stars or dots to indicate how many keys you have pressed. Keys are still being registered, however, so just type the password and press enter. \n",
    "\n",
    "Finally, we can start the turtlebot navigation stack launch file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!roslaunch workshop_tutorial turtlebot_navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You will now see an rviz window similar to the SLAM section. The occupancy grid map is displayed just like before, but some extra information is added. The mean estimated location of the robot is shown as a small robot model. Other pose estimate particles (which are tracked by [AMCL](http://wiki.ros.org/amcl)) are displayed with green dots. The blue/purple/red blobs indicate the location of detected obstacles in the robot frame. \n",
    "\n",
    "![navigation_example](images/navigation_example.png)\n",
    "\n",
    "Most likely the initial position estimate will not be correct, and the detected obstacles will not be aligned with the map. Before starting the navigation, this initial estimate should be corrected. \n",
    "\n",
    "1. At the top of the rviz window, click the '2D Pose Estimate button'.\n",
    "2. Click on the approximate point in the map where the TurtleBot3 is located and drag the cursor to indicate the direction in which it faces.\n",
    "\n",
    "Repeat this proces if necessary untill the detected obstacles are approximately aligned with the map. Once the Turtlebot initial pose is set, a goal pose can be set in a similar manner with the '2D Nav Goal' button. A trajectory will now be planned, and the robot will start driving. As it moves, the spread of pose estimates (the green dots) will become smaller as the particle filter converges. If the robot encounters an unplanned obstacle (such as people walking around), then it will recompute a new trajectory.\n",
    "\n",
    "When you are done, you can close all processes with CTRL+C (you might need to press is more then once for large launch files such as the navigation file). Before closing the SSH connection (the terminal that starts with rosberrypi@rosberrypi) shut down the pi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sudo shutdown now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After about 1 minute, it is safe to turn the robot off with the power switch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "337px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
